apiVersion: workload.serving.volcano.sh/v1alpha1
kind: ModelServing
metadata:
  name: my-model
  namespace: kthena-system
spec:
  schedulerName: volcano
  replicas: 1
  template:
    gangPolicy:
      minRoleReplicas:
        leader: 1
    restartGracePeriodSeconds: 60
    roles:
      - entryTemplate:
          spec:
            containers:
              - command:
                  - python3
                  - -m
                  - vllm.entrypoints.openai.api_server
                  - --model
                  - /tmp/cache/Qwen3
                  - --data-parallel-address
                  - my-model-0-leader-0-0.kthena-system.svc.cluster.local # Use the internal address of pod to communicate.
                  - --data-parallel-rank
                  - "0"
                  - --data-parallel-rpc-port
                  - "13345"
                  - --data-parallel-size
                  - "2"
                  - --enforce-eager
                  - --gpu-memory-utilization
                  - "0.9"
                  - --max-model-len
                  - "2048"
                  - --max-num-seqs
                  - "16"
                  - --served-model-name
                  - my-model
                env:
                  - name: POD_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.name
                  - name: NAMESPACE
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.namespace
                  - name: VLLM_USE_V1
                    value: "1"
                image: vllm/vllm-openai:v0.13.0
                name: engine
                readinessProbe:
                  failureThreshold: 3
                  httpGet:
                    path: /health
                    port: 8000
                    scheme: HTTP
                  initialDelaySeconds: 180
                  periodSeconds: 5
                  successThreshold: 1
                  timeoutSeconds: 1
                resources:
                  limits:
                    nvidia.com/gpu: "1"
                volumeMounts:
                  - mountPath: /tmp/cache
                    name: example-weights
                  - mountPath: /dev/shm
                    name: dshm
            initContainers:
              - args:
                  - --source
                  - hf://Qwen/Qwen3-0.6B
                  - --output-dir
                  - /tmp/cache/Qwen3
                image: ghcr.io/volcano-sh/downloader:v0.2.0
                name: my-model-model-downloader
                resources: {}
                volumeMounts:
                  - mountPath: /tmp/cache
                    name: example-weights
            terminationGracePeriodSeconds: 300
            volumes:
              - hostPath:
                  path: /tmp/cache
                  type: DirectoryOrCreate
                name: example-weights
              - emptyDir:
                  medium: Memory
                name: dshm
        name: leader
        replicas: 1
        workerReplicas: 1
        workerTemplate:
          spec:
            containers:
              - command:
                  - python3
                  - -m
                  - vllm.entrypoints.openai.api_server
                  - --model
                  - /tmp/cache/4005c13ab5d91cb094e41048356abe39
                  - --data-parallel-address
                  - my-model-0-leader-0-0.kthena-system.svc.cluster.local
                  - --data-parallel-rank
                  - "1"
                  - --data-parallel-rpc-port
                  - "13345"
                  - --data-parallel-size
                  - "2"
                  - --enforce-eager
                  - --gpu-memory-utilization
                  - "0.9"
                  - --max-model-len
                  - "2048"
                  - --max-num-seqs
                  - "16"
                  - --served-model-name
                  - my-model
                image: vllm/vllm-openai:v0.13.0
                name: example-vllm-worker
                resources:
                  limits:
                    nvidia.com/gpu: "1"
                volumeMounts:
                  - mountPath: /tmp/cache
                    name: example-weights
                  - mountPath: /dev/shm
                    name: dshm
            initContainers:
              - args:
                  - --source
                  - hf://Qwen/Qwen3-0.6B
                  - --output-dir
                  - /tmp/cache/Qwen3
                image: ghcr.io/volcano-sh/downloader:v0.2.0
                name: my-model-model-downloader
                resources: {}
                volumeMounts:
                  - mountPath: /tmp/cache
                    name: example-weights
            volumes:
              - hostPath:
                  path: /tmp/cache
                  type: DirectoryOrCreate
                name: example-weights
              - emptyDir:
                  medium: Memory
                name: dshm
